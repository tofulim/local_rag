{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'cmpl-c82abe22d03d46fd83ca138b7f25983d', 'object': 'text_completion', 'created': 1738555557, 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', 'choices': [{'index': 0, 'text': '!!!!!!!!!!!!!!!!', 'logprobs': None, 'finish_reason': 'length', 'stop_reason': None, 'prompt_logprobs': None}], 'usage': {'prompt_tokens': 4, 'total_tokens': 20, 'completion_tokens': 16, 'prompt_tokens_details': None}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "vllm_host = \"http://localhost:8000\"\n",
    "url = f\"{vllm_host}/v1/completions\"\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    \"prompt\": [\n",
    "        \"hi there!\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "print(response.json()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.vllm import SamplingParams\n",
    "from vllm.vllm import LLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    dtype=\"float32\",\n",
    "    max_model_len=49028,\n",
    "    # device=\"mps\",\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "# Prepare your prompts\n",
    "base_prompt = \"\"\"\n",
    "You are designed to answer user questions accurately and effectively.\n",
    "Reference documents will be provided and if the references contain useful information then use it. \n",
    "But using them is not mendatory, if they are not relevant you could ignore them.\n",
    "Also Your response should be short, compact, clear, well-structured, compact and informative.\n",
    "\"\"\"\n",
    "cites = \"\"\"\n",
    "Now, here are some hint Documents.\n",
    "1. Doc1: Israel Adesanya fights in the middleweight division and was the middleweight champion from 2019 to 2022.\n",
    "2. Doc: 2024 UFC middleweight champion is Dricus du Plessis. and he got his next fight at UFC 312 fight card with his contender Sean Strickland.\n",
    "2. Doc3: In the wake of Yoel Romeroâ€™s stunning knock out victory against Luke Rockholdat UFC 221, there are a number of possible matchups at 2021\n",
    "\n",
    "Please answer the question.\n",
    "\"\"\"\n",
    "prompt = f\"who is the 2024 UFC middleweight champion?{cites}\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. {base_prompt}\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "query = \"who is the 2024 UFC middleweight champion?\"\n",
    "\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": base_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": cites,\n",
    "    },\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens=64)\n",
    "\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "output = llm.chat(conversation, sampling_params=sampling_params)\n",
    "end = time()\n",
    "# text='</think>\\n\\nThe 2024 UFC middleweight champion is Dricus'\n",
    "print(output)\n",
    "print(f\"time elapsed {end - start}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
